model:
  vocab_size: null  # Set at runtime
  d_model: 512
  num_heads: 8
  num_layers: 6
  sequence_length: 256
  dropout: 0.1
  ffn_expansion: 4
  pos_encoding: learned

training:
  data_path: data/processed/text.txt
  train_split: 0.9
  sequence_length: 256
  batch_size: 32
  num_workers: 4

  max_epochs: 10
  max_steps: null
  gradient_accumulation_steps: 1
  gradient_clip_norm: 1.0

  optimizer: adamw
  learning_rate: 3.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8

  lr_scheduler: cosine
  warmup_steps: 100
  min_lr_ratio: 0.1

  use_mixed_precision: true
  dropout: 0.1

  checkpoint_dir: checkpoints/
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3

  early_stopping_patience: 5
  early_stopping_metric: val_loss
  early_stopping_mode: min

  log_every_n_steps: 100
  eval_every_n_epochs: 1

  seed: 42
  deterministic: true
  device: cuda
